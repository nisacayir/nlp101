{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Tokenizasyon (Tokenleştirme)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\nTokenizasyon, metin verileriyle çalışırken uygulanan en temel adımlardan biridir. Token, kelime veya duruma göre cümle anlamına gelebilir. Tokenizasyon, bir metni tokenlarına yani kelimelerine veya cümlelerine ayırma işlemidir. Metinler genellikle kelimelerine ayrılır, ancak duruma göre cümlelerine ayırmak da gerekebilir.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Neden Tokenizasyon Yapılır?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1. **Dil Yapısını Anlama**: Metin, anlamlı parçalarına (kelimelerine) ayrıldığında dil yapısı daha net anlaşılabilir.\n2. **Frekans Analizi**: Belirli kelimelerin bir metinde ne sıklıkla geçtiğini görmemizi sağlar, bu sayede metnin genel konusu anlaşılabilir.\n3. **Metin Temizleme**: Gereksiz veya istenmeyen karakterleri (noktalama işaretleri, özel karakterler, bağlaçlar vb.) kaldırarak metni daha temiz hale getirir.\n4. **Özellik Çıkarımı**: Metinden özellik çıkarımı yapmamızı sağlar. Örneğin, belirli kelimelerin varlığı, metnin bir sınıfa ait olup olmadığını belirlememize yardımcı olabilir.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 1. Split Metodu ile Tokenizasyon",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Python'da tokenizasy on yapmanın en basit yöntemlerinden biri `split()` fonksiyonudur. Bu fonksiyon, metni boşluklara göre tokenlara ayırır.\n**Not:** `split()` metodu noktalama işaretlerini dikkate almaz.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Makine öğrenmesiyle ilgili metin\ntext = \"\"\"Makine öğrenmesi, yapay zekanın bir alt dalıdır ve bilgisayarların verilerden öğrenmesini sağlar. \n            Bu süreçte, algoritmalar verileri analiz eder ve bu verilerden çıkarımlar yaparak tahminlerde bulunur. \n            Örneğin, bir makine öğrenmesi modeli, müşteri davranışlarını analiz ederek gelecekteki satın alma eğilimlerini tahmin edebilir.\"\"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "# split() fonksiyonu ile tokenizasyon\ntokens = text.split()\n\nprint(tokens)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['Makine', 'öğrenmesi,', 'yapay', 'zekanın', 'bir', 'alt', 'dalıdır', 've', 'bilgisayarların', 'verilerden', 'öğrenmesini', 'sağlar.', 'Bu', 'süreçte,', 'algoritmalar', 'verileri', 'analiz', 'eder', 've', 'bu', 'verilerden', 'çıkarımlar', 'yaparak', 'tahminlerde', 'bulunur.', 'Örneğin,', 'bir', 'makine', 'öğrenmesi', 'modeli,', 'müşteri', 'davranışlarını', 'analiz', 'ederek', 'gelecekteki', 'satın', 'alma', 'eğilimlerini', 'tahmin', 'edebilir.']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "## 2. RegEx ve NLTK ile Tokenizasyon",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Regex, belirli bir deseni (pattern) aramak veya metin üzerinde işlemler yapmak için kullanılan bir araçtır. Daha gelişmiş tokenizasyon işlemleri için regex kullanılabilir.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import re\n\n# metin (yapay zeka ve veri bilimi hakkında)\ntext = \"\"\"Yapay zeka, veri biliminin en önemli alanlarından biridir. \n            Makine öğrenmesi modelleri, büyük veri setleri üzerinde eğitilerek tahminlerde bulunur. \n            Derin öğrenme ise sinir ağları kullanarak daha karmaşık problemleri çözmeyi hedefler.\"\"\"\n\n# Regex pattern (aynı kalacak)\npattern = r'''(?x)\n[A-Z]\\.+\n| \\w+(?:-\\w+)*\n| \\$?\\d+(?:\\.\\d+)?%?\n| \\.\\.\\.\n| [][.,;\"'?():-_`]\n'''\n\n# Regex ile tokenizasyon\ntokens_regex = re.findall(pattern, text)\nprint(\"Tokens Regex :\", tokens_regex)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Tokens Regex : ['Yapay', 'zeka', ',', 'veri', 'biliminin', 'en', 'önemli', 'alanlarından', 'biridir', '.', 'Makine', 'öğrenmesi', 'modelleri', ',', 'büyük', 'veri', 'setleri', 'üzerinde', 'eğitilerek', 'tahminlerde', 'bulunur', '.', 'Derin', 'öğrenme', 'ise', 'sinir', 'ağları', 'kullanarak', 'daha', 'karmaşık', 'problemleri', 'çözmeyi', 'hedefler', '.']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": "NLTK (Natural Language Toolkit), Python için popüler bir NLP kütüphanesidir. NLTK'nin regexp_tokenize() fonksiyonu, regex pattern'lerini destekler.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import nltk\n\ntokens_nltk = nltk.regexp_tokenize(text, pattern)\nprint(\"\\nTokens NLTK :\", tokens_nltk)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nTokens NLTK : ['Yapay', 'zeka', ',', 'veri', 'biliminin', 'en', 'önemli', 'alanlarından', 'biridir', '.', 'Makine', 'öğrenmesi', 'modelleri', ',', 'büyük', 'veri', 'setleri', 'üzerinde', 'eğitilerek', 'tahminlerde', 'bulunur', '.', 'Derin', 'öğrenme', 'ise', 'sinir', 'ağları', 'kullanarak', 'daha', 'karmaşık', 'problemleri', 'çözmeyi', 'hedefler', '.']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": "## 3. NLTK ile Hazır Tokenizasyon",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "NLTK'nin punkt modeli, dilbilgisel kuralları ve örüntüleri içeren önceden eğitilmiş bir modeldir. Türkçe dahil birçok dil için tokenizasyon işlemi yapabilir.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#!pip install nltk\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nnltk.download('punkt')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#metin TR\ntr_text = \"\"\"Veri bilimi, büyük veri setlerini analiz ederek anlamlı bilgiler çıkarmayı amaçlar. \n             Bu alanda, istatistik, makine öğrenmesi ve veri görselleştirme gibi teknikler kullanılır. \n             Veri bilimciler, analiz eder ve iş dünyasına yönelik çözümler üretir.\"\"\"\n\n# Kelime tokenizasyonu\ntr_tokens = word_tokenize(tr_text)\nprint(\"Türkçe Kelime Tokenları:\", tr_tokens)\n\n# Cümle tokenizasyonu\ntr_sent_tokens = sent_tokenize(tr_text)\nprint(\"\\nTürkçe Cümle Tokenları:\", tr_sent_tokens)",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# metin EN\nen_text = \"Socrates is one of the famous philosophers of the ancient era. One of his most famous quotes is: 'The only thing I know is that I know nothing.'\"\n\n# Kelime tokenizasyonu\nen_tokens = word_tokenize(en_text)\nprint(\"İngilizce Kelime Tokenları:\", en_tokens)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Metin Temizleme ve Stop Words Kaldırma",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Noktalama işaretleri ve stop words'lerden arındırılmış bir tokenizasyon işlemi için aşağıdaki fonksiyon kullanılabilir.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom string import punctuation\n\n# Ön işleme fonksiyonu\ndef preprocess_corpus(texts):\n    mystopwords = set(stopwords.words('turkish'))\n\n    def remove_stops_digits(tokens):\n        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit() and token not in punctuation]\n    \n    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n\n# metin\ntext = \"\"\"Yapay zeka, bilgisayarların insan benzeri kararlar almasını sağlayan bir teknolojidir. \n          Makine öğrenmesi, yapay zekanın bir alt dalıdır ve verilerden otomatik olarak öğrenme yeteneğini ifade eder. \n          Derin öğrenme ise sinir ağları kullanarak karmaşık problemleri çözmeyi hedefler.\"\"\"\n\n# Metni ön işleme adımlarından geçirme\npreprocessed_text = preprocess_corpus([text])\nprint(\"Temizlenmiş Tokenlar:\", preprocessed_text)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Kaynaklar\nNatural Language Processing with Python (O’Reilly)\nPractical Natural Language Processing (O’Reilly)",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "Bu dokümanı Jupyter Notebook olarak kaydedebilir ve GitHub'da paylaşabilirsiniz.  Her bir başlık ve kod bloğu, Jupyter'ın hücre yapısına uygun şekilde düzenlenmiştir.",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    }
  ]
}