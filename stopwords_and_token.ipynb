{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords ve Tokenizasyon İşlemleri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu dokümanda, metin işleme süreçlerinde sıkça kullanılan stopwords temizleme ve tokenizasyon işlemlerini ele alacağım. Ayrıca, bu işlemlerin neden önemli olduğunu ve nasıl uygulandığını örneklerle açıklayacağım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stopwords Nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords, metinlerde sıkça geçen ancak tek başına anlam ifade etmeyen kelimelerdir. Örneğin:\n",
    "\n",
    "Türkçe: \"ama\", \"ve\", \"bir\", \"gibi\"\n",
    "\n",
    "İngilizce: \"the\", \"is\", \"in\", \"and\"\n",
    "\n",
    "Stopwords'ler, metin analizi sırasında genellikle temizlenir çünkü bu kelimeler analizin odaklanması gereken daha anlamlı kelimeleri gölgeleyebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NLTK ile Stopwords Temizleme:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit), stopwords'leri temizlemek için kullanılan popüler bir Python kütüphanesidir. Aşağıda, Türkçe ve İngilizce metinlerde stopwords temizleme işlemi örnekleri bulunmaktadır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. NLTK Stopwords İndirme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Öncelikle, NLTK'nin stopwords veri setini indirmemiz gerekiyor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Türkçe Stopwords: ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# türkçe stopwords listesi\n",
    "turkce_stopwords = stopwords.words('turkish')\n",
    "print(\"Türkçe Stopwords:\", turkce_stopwords[:10])  # ilk 10 örnek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İngilizce Stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# İngilizce stopwords listesi\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(\"İngilizce Stopwords:\", english_stopwords[:10])  # İlk 10 örnek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stopwords Temizleme Fonksiyonu;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords'leri temizlemek için bir fonksiyon yazalım. Bu fonksiyon, metni tokenlara ayırır ve stopwords'leri kaldırır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# punkt modelini yeniden indir\n",
    "nltk.download('punkt', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize # type: ignore\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords temizleme fonksiyonu\n",
    "def remove_stopwords_turkish(text):\n",
    "    stop_words = set(stopwords.words('turkish'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Örnek metin\n",
    "turkish_text = \"\"\"At yarışları, binicilik sporunun en popüler dallarından biridir. \n",
    "                  Yarış atları, hızları ve çeviklikleri ile ünlüdür. \n",
    "                  Bu yarışlar, hem atların hem de binicilerin yeteneklerini sergilediği bir platform sunar. \n",
    "                  Ayrıca, at yarışları, tarih boyunca kültürel ve sosyal etkinliklerin bir parçası olmuştur.\"\"\"\n",
    "\n",
    "# Stopwords temizleme\n",
    "filtered_text = remove_stopwords_turkish(turkish_text)\n",
    "print(\"Temizlenmiş Metin:\", filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Anlamlı İçerik Oranı"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir metnin ne kadarının anlamlı içerik taşıdığını ölçmek için stopwords'lerin oranını hesaplayabiliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Türkçe Metin İçin Anlamlı İçerik Oranı:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_fraction_turkish(text):\n",
    "    stop_words = set(stopwords.words('turkish'))\n",
    "    tokens = word_tokenize(text)\n",
    "    content = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return len(content) / len(tokens)\n",
    "\n",
    "# Örnek metin\n",
    "turkish_text = \"Sokrates, antik çağın ünlü filozoflarından biridir. En ünlü sözlerinden biri şöyledir: 'Bilgiğim bir şey var, o da hiçbir şey bilmediğimdir.'\"\n",
    "\n",
    "# Anlamlı içerik oranı\n",
    "ratio = content_fraction_turkish(turkish_text)\n",
    "print(\"Anlamlı İçerik Oranı:\", ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anlamlı İçerik Oranı: 0.9716312056737588"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 İngilizce Metinler İçin Anlamlı İçerik Oranı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_fraction_english(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    content = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return len(content) / len(tokens)\n",
    "\n",
    "# Örnek metin\n",
    "english_text = \"Socrates is one of the famous philosophers of the ancient era. One of his most famous quotes is: 'The only thing I know is that I know nothing.'\"\n",
    "\n",
    "# Anlamlı içerik oranı\n",
    "ratio = content_fraction_english(english_text)\n",
    "print(\"Anlamlı İçerik Oranı:\", ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anlamlı İçerik Oranı: 0.5902777777777778"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sonuç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu dokümanda, stopwords temizleme ve tokenizasyon işlemlerini NLTK kütüphanesi kullanarak nasıl gerçekleştirebileceğimizi öğrendim. Bu işlemler, metin analizi ve doğal dil işleme süreçlerinde büyük önem taşır. Özellikle büyük veri setlerinde, stopwords'leri temizlemek işlem süresini kısaltır ve analizin kalitesini artırır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaynaklar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org \n",
    "Natural Language Processing with Python (O’Reilly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu dokümanı Jupyter Notebook olarak kaydedebilir ve GitHub'da paylaşabilirsiniz.  Her bir başlık ve kod bloğu, Jupyter'ın hücre yapısına uygun şekilde düzenlenmiştir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
